<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="CNN 池化的作用类似于人眼的降采样，减少特征量，进一步提取高维特征  CV任务和Speech任务、NLP任务的相关性在于, 都存在局部与整体的关系, 由低层次的特征经过组合, 组成高层次的特征, 并且得到不同特征之间的空间相关性.    而CNN刚好可以通过卷积，池化等操作实现了数据降维与特征的连接。 局部相关性和权值共享局部相关性和权值共享：通过利用局部相关性避免全连接网络的参数集过大的缺点。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习的一些理解">
<meta property="og:url" content="http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/index.html">
<meta property="og:site_name" content="Haiyu&#39;s space">
<meta property="og:description" content="CNN 池化的作用类似于人眼的降采样，减少特征量，进一步提取高维特征  CV任务和Speech任务、NLP任务的相关性在于, 都存在局部与整体的关系, 由低层次的特征经过组合, 组成高层次的特征, 并且得到不同特征之间的空间相关性.    而CNN刚好可以通过卷积，池化等操作实现了数据降维与特征的连接。 局部相关性和权值共享局部相关性和权值共享：通过利用局部相关性避免全连接网络的参数集过大的缺点。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/images/20221015_1.jpeg">
<meta property="article:published_time" content="2022-10-13T14:45:04.000Z">
<meta property="article:modified_time" content="2022-10-16T09:33:43.099Z">
<meta property="article:author" content="Haiyu Zhang">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/images/20221015_1.jpeg">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>深度学习的一些理解</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Haiyu's space" type="application/atom+xml">
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About me</a></li><!--
     --><!--
       --><li><a href="/archives/">Articles</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" aria-label="Next post" href="/2022/10/12/%E5%9F%BA%E4%BA%8E%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E7%9A%84%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&text=深度学习的一些理解"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&is_video=false&description=深度学习的一些理解"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=深度学习的一些理解&body=Check out this article: http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&name=深度学习的一些理解&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&t=深度学习的一些理解"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN"><span class="toc-number">1.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E7%9B%B8%E5%85%B3%E6%80%A7%E5%92%8C%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB"><span class="toc-number">1.1.</span> <span class="toc-text">局部相关性和权值共享</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">3.</span> <span class="toc-text">训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#batch-normalization"><span class="toc-number">3.0.1.</span> <span class="toc-text">batch normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet"><span class="toc-number">3.0.2.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Weights-Init%E7%9A%84%E6%96%B9%E5%BC%8F%E5%92%8C%E5%A5%BD%E5%A4%84"><span class="toc-number">3.0.3.</span> <span class="toc-text">Weights Init的方式和好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%BD%91%E7%BB%9C%E5%A4%9F%E6%B7%B1-Neurons-%E8%B6%B3%E5%A4%9F%E5%A4%9A-%E7%9A%84%E6%97%B6%E5%80%99-%E6%80%BB%E6%98%AF%E5%8F%AF%E4%BB%A5%E9%81%BF%E5%BC%80%E8%BE%83%E5%B7%AELocal-Optima"><span class="toc-number">3.0.4.</span> <span class="toc-text">为什么网络够深(Neurons 足够多)的时候, 总是可以避开较差Local Optima?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%AE%9A%E4%B9%89%E6%96%B9%E5%BC%8F-%E5%9F%BA%E4%BA%8E%E4%BB%80%E4%B9%88-%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F-%E6%80%8E%E4%B9%88%E4%BC%98%E5%8C%96-%E5%90%84%E8%87%AA%E7%9A%84%E5%A5%BD%E5%A4%84-%E4%BB%A5%E5%8F%8A%E8%A7%A3%E9%87%8A"><span class="toc-number">3.0.5.</span> <span class="toc-text">Loss. 有哪些定义方式(基于什么? ),  有哪些优化方式, 怎么优化, 各自的好处, 以及解释.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-%E6%80%8E%E4%B9%88%E5%81%9A-%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%E5%A4%84-%E8%A7%A3%E9%87%8A"><span class="toc-number">3.0.6.</span> <span class="toc-text">Dropout.  怎么做, 有什么用处, 解释.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Activation-Function-%E9%80%89%E7%94%A8%E4%BB%80%E4%B9%88-%E6%9C%89%E4%BB%80%E4%B9%88%E5%A5%BD%E5%A4%84-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E8%BF%99%E6%A0%B7%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">3.0.7.</span> <span class="toc-text">Activation Function. 选用什么, 有什么好处, 为什么会有这样的好处.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD"><span class="toc-number">3.0.8.</span> <span class="toc-text">SGD</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        深度学习的一些理解
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Haiyu Zhang</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2022-10-13T14:45:04.000Z" itemprop="datePublished">2022-10-13</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><ul>
<li><p>池化的作用类似于人眼的降采样，减少特征量，进一步提取高维特征</p>
</li>
<li><p>CV任务和Speech任务、NLP任务的相关性在于, 都存在局部与整体的关系, 由低层次的特征经过组合, 组成高层次的特征, 并且得到不同特征之间的空间相关性. </p>
</li>
</ul>
<p>而CNN刚好可以通过卷积，池化等操作实现了数据降维与特征的连接。</p>
<h2 id="局部相关性和权值共享"><a href="#局部相关性和权值共享" class="headerlink" title="局部相关性和权值共享"></a>局部相关性和权值共享</h2><p>局部相关性和权值共享：<br>通过利用局部相关性避免全连接网络的参数集过大的缺点。<br>以图片为例，对于2D图片通常在进入全连接层之前将矩阵数据打平成1D向量，然后每个像素点与每个输出节点两两相连。网络层中，每个输出节点都与所有的输入节点相连，这种稠密的（Dense）连接方式使得全连接层的参数量大，计算代价高，全连接层也叫做稠密层（Dense Layer），当全连接层的激活函数为空时，全连接层也叫做线性层（Linear Layer）。<br>其实，对于图片而言只要选出最重要的一部分输入节点，抛弃重要性较低的节点<br>二维平面通常将位置平面作为重要性分布衡量标准的依据，距离越近，重要性越大，影响越大；距离越远，重要性越小<br>引入：感受野（Receptive field），他表征了每个像素点杜宇中心像素的重要性分布，网格内的像素才会被考虑到，网格外会被简单的忽略。</p>
<p>权值共享：<br>通过局部相关性减少了参数量，还可以通过权值共享使得参数量得到进一步的减小。权值共享就是对于输出节点都使用相同的权值矩阵 。那么，无论输出节点的数量，网络层的参数量总是固定。<br>通过局部相关性和权值共享的思想就可以将网络参数量明显的减小（对于单输入通道，单卷积核的条件下。）这种拒不连接、共享权值的方法就是卷积神经网络。</p>
<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><ul>
<li>数据集太小时，深度学习的优势就体现不出。数据集没有局部相关性的话，深度学习的表现也不会好。</li>
</ul>
<h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><ul>
<li>神经网络训练中，一般是通过BP算法使得网络输出尽可能的接近标签，如果梯度消失会导致权值无法更新，训练难度加大。梯度消失主要是因为Back Propagation 算法主要为多个激活函数求导想乘，如果一个激活函数的求导小于1，随着层数加深，会导致整个结果接近0. 梯度消失<br><img src="images/20221015_1.jpeg" alt="sigmoid "></li>
<li>梯度爆炸和消失的避免方式有很多，可以用ReLu函数，因为导数为1。或者目前有广泛使用的Batch Normalization<h3 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h3>应用每层激活函数之前, 就是做均值和方差归一化, 对于每一批次数据并且还做放大缩小, 平移。 为了梯度下降的收敛速度更快, 相当于把数据都拉到中间的位置了, 有这个就不需要Dropout, Relu等等. BN使得每层输出信号满足均值为0, 方差为1的分布, 而最后的”scale and shift”操作则是为了让因训练所需而”刻意”加入的BN能够有可能还原最初的输入, 从而保证整个网络的稳定性. 简而言之, BN包括两点: 归一化+缩放平移</li>
</ul>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><ul>
<li>残差网络可以解决两个问题，一个是随着网络层数的增加, 网络的退化(degradation)的现象，另一个是解决梯度消失。</li>
<li>网络退化：随着网络层数的增多, 训练集loss逐渐下降, 然后趋于饱和, 当再增加网络深度, 训练集loss反而会增大. 注意这并不是过拟合, 因为在过拟合中训练loss是一直减小的. </li>
<li>ResNet直接映射的加入保证 L 层不少于浅层 l 层的特征信息。而且L 层的梯度可以直接传递到任何一个比它浅的 l 层, 不经过中间层的权重矩阵. 信息可以非常畅通的在高层和低层之间相互传导. </li>
</ul>
<h3 id="Weights-Init的方式和好处"><a href="#Weights-Init的方式和好处" class="headerlink" title="Weights Init的方式和好处"></a>Weights Init的方式和好处</h3><h3 id="为什么网络够深-Neurons-足够多-的时候-总是可以避开较差Local-Optima"><a href="#为什么网络够深-Neurons-足够多-的时候-总是可以避开较差Local-Optima" class="headerlink" title="为什么网络够深(Neurons 足够多)的时候, 总是可以避开较差Local Optima?"></a>为什么网络够深(Neurons 足够多)的时候, 总是可以避开较差Local Optima?</h3><p>The Loss Surfaces of Multilayer Networks</p>
<h3 id="Loss-有哪些定义方式-基于什么-有哪些优化方式-怎么优化-各自的好处-以及解释"><a href="#Loss-有哪些定义方式-基于什么-有哪些优化方式-怎么优化-各自的好处-以及解释" class="headerlink" title="Loss. 有哪些定义方式(基于什么? ),  有哪些优化方式, 怎么优化, 各自的好处, 以及解释."></a>Loss. 有哪些定义方式(基于什么? ),  有哪些优化方式, 怎么优化, 各自的好处, 以及解释.</h3><p>Cross-Entropy / MSE / K-L散度</p>
<h3 id="Dropout-怎么做-有什么用处-解释"><a href="#Dropout-怎么做-有什么用处-解释" class="headerlink" title="Dropout.  怎么做, 有什么用处, 解释."></a>Dropout.  怎么做, 有什么用处, 解释.</h3><p>How does the dropout method work in deep learning?<br>Improving neural networks by preventing co-adaptation of feature detectors<br>An empirical analysis of dropout in piecewise linear networks</p>
<h3 id="Activation-Function-选用什么-有什么好处-为什么会有这样的好处"><a href="#Activation-Function-选用什么-有什么好处-为什么会有这样的好处" class="headerlink" title="Activation Function. 选用什么, 有什么好处, 为什么会有这样的好处."></a>Activation Function. 选用什么, 有什么好处, 为什么会有这样的好处.</h3><p>几种主要的激活函数: Sigmond / ReLU ／PReLU</p>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>SGD 中 S(stochastic)代表什么?<br>1.使用整个训练集的优化算法被称为批量(batch)或确定性(deterministic)梯度算法, 因为会在一个大批量中同时处理所有样本. </p>
<ol>
<li>每次只使用单个样本的优化算法有时被称为随机(stochastic)或者在线(online)算法.<br>大多数用于深度学习的算法介于两者之间, 使用一个以上而不是全部的训练样本. 现在通常将他们简单地称为随机(stochastic)方法. </li>
</ol>
<p>摘自<deep learning>中文版171页. </deep></p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About me</a></li>
         
          <li><a href="/archives/">Articles</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN"><span class="toc-number">1.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E7%9B%B8%E5%85%B3%E6%80%A7%E5%92%8C%E6%9D%83%E5%80%BC%E5%85%B1%E4%BA%AB"><span class="toc-number">1.1.</span> <span class="toc-text">局部相关性和权值共享</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">3.</span> <span class="toc-text">训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#batch-normalization"><span class="toc-number">3.0.1.</span> <span class="toc-text">batch normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet"><span class="toc-number">3.0.2.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Weights-Init%E7%9A%84%E6%96%B9%E5%BC%8F%E5%92%8C%E5%A5%BD%E5%A4%84"><span class="toc-number">3.0.3.</span> <span class="toc-text">Weights Init的方式和好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%BD%91%E7%BB%9C%E5%A4%9F%E6%B7%B1-Neurons-%E8%B6%B3%E5%A4%9F%E5%A4%9A-%E7%9A%84%E6%97%B6%E5%80%99-%E6%80%BB%E6%98%AF%E5%8F%AF%E4%BB%A5%E9%81%BF%E5%BC%80%E8%BE%83%E5%B7%AELocal-Optima"><span class="toc-number">3.0.4.</span> <span class="toc-text">为什么网络够深(Neurons 足够多)的时候, 总是可以避开较差Local Optima?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%AE%9A%E4%B9%89%E6%96%B9%E5%BC%8F-%E5%9F%BA%E4%BA%8E%E4%BB%80%E4%B9%88-%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F-%E6%80%8E%E4%B9%88%E4%BC%98%E5%8C%96-%E5%90%84%E8%87%AA%E7%9A%84%E5%A5%BD%E5%A4%84-%E4%BB%A5%E5%8F%8A%E8%A7%A3%E9%87%8A"><span class="toc-number">3.0.5.</span> <span class="toc-text">Loss. 有哪些定义方式(基于什么? ),  有哪些优化方式, 怎么优化, 各自的好处, 以及解释.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-%E6%80%8E%E4%B9%88%E5%81%9A-%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%E5%A4%84-%E8%A7%A3%E9%87%8A"><span class="toc-number">3.0.6.</span> <span class="toc-text">Dropout.  怎么做, 有什么用处, 解释.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Activation-Function-%E9%80%89%E7%94%A8%E4%BB%80%E4%B9%88-%E6%9C%89%E4%BB%80%E4%B9%88%E5%A5%BD%E5%A4%84-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E8%BF%99%E6%A0%B7%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">3.0.7.</span> <span class="toc-text">Activation Function. 选用什么, 有什么好处, 为什么会有这样的好处.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD"><span class="toc-number">3.0.8.</span> <span class="toc-text">SGD</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&text=深度学习的一些理解"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&is_video=false&description=深度学习的一些理解"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=深度学习的一些理解&body=Check out this article: http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&title=深度学习的一些理解"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&name=深度学习的一些理解&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://haiyu-haiyuzhang.github.io/2022/10/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/&t=深度学习的一些理解"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
      <div class="footer-left">
        Copyright &copy;
        
        
        2019-2022
        Haiyu Zhang
      </div>
      <div class="footer-right">
        <nav>
          <ul>
            <!--
          --><li><a href="/">Home</a></li><!--
        --><!--
          --><li><a href="/about/">About me</a></li><!--
        --><!--
          --><li><a href="/archives/">Articles</a></li><!--
        --><!--
          --><li><a href="/search/">Search</a></li><!--
        -->
          </ul>
          <ul>
            
              <!-- 不蒜子统计 -->
              <span id="busuanzi_container_site_pv">
                  本站总访问量<span id="busuanzi_value_site_pv"></span>次
              </span>
              <span class="post-meta-divider">|</span>
              <span id="busuanzi_container_site_uv" style='display:none'>
                      本站访客数<span id="busuanzi_value_site_uv"></span>人
              </span>
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            
          </ul>
        </nav>
      </div>
      
</footer>
    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
